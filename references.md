
# References

<a id="GDPR2018">[1]</a> 
European Commission. 2018.
2018 reform of EU data protection rules. 
[https://ec.europa.eu/commission/sites/beta-political/files/data-protection-factsheet-changes_en.pdf](https://ec.europa.eu/commission/sites/beta-political/files/data-protection-factsheet-changes_en.pdf)

<a id="fjeld2020">[2]</a> 
Jessica Fjeld, Nele Achten, Hannah Hilligoss, Adam Nagy, Madhulika Srikumar. 2020.
Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-Based Approaches to Principles for AI. 
SSRN Electronic Journal (2020) [https://doi.org/10.2139/ssrn.3518482](https://doi.org/10.2139/ssrn.3518482)

<a id="krafft2020">[3]</a> 
AI Ethics Impact Group (AIEIG). 2020. 
From Principles to Practice An interdisciplinary framework to operationalise AI ethics. 
[https://www.ai-ethics-impact.org/resource/blob/1961130/c6db9894ee73aefa489d6249f5ee2b9f/aieig---report---download-hb-data.pdf](https://www.ai-ethics-impact.org/resource/blob/1961130/c6db9894ee73aefa489d6249f5ee2b9f/aieig---report---download-hb-data.pdf)

<a id="floridi2019">[4]</a> 
Luciano Floridi. 2019. 
Translating Principles into Practices of Digital Ethics: Five Risks of Being Unethical. 
Philosophy & Technology 32, 2 (6 2019). [https://doi.org/10.1007/s13347-019-00354-x](https://doi.org/10.1007/s13347-019-00354-x)

<a id="IEEE2019">[5]</a> 
IEEE. 2019.
The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems.
Ethically Aligned Design: A Vision for Prioritizing Human Well-being with Autonomous and Intelligent Systems (first edition ed.).

<a id="mehldau2007">[6]</a> 
Matthias Mehldau. 2007.
Iconset for data-privacy declarations v 0.1.
Ethically Aligned Design: A Vision for Prioritizing Human Well-being with Autonomous and Intelligent Systems (first edition ed.).

<a id="blazevic2021">[7]</a> 
Alice Namuli Blazevic, Patrick Mugalula, and Andrew Wandera. 2021. 
Towards Operationalizing the Data Protection and Privacy Act 2020 Understanding the Draft Data Protection and Privacy Regulations. 
SSRN Electronic Journal (2021). [https://doi.org/10.2139/ssrn.3776353](https://doi.org/10.2139/ssrn.3776353)

<a id="microsoftai2018">[8]</a> 
Microsoft. 2018. 
AI Principles.
[https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1%3aprimaryr6](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1%3aprimaryr6)

<a id="hidalgo2021">[9]</a> 
César Hidalgo, Diana Orghian, Jordi Albo-Canals, Filipa de Almeida, and Natalia Martin. 2021. 
How Humans Judge Machines. 
MIT Press. [https://hal.archives-ouvertes.fr/hal-03058652](https://hal.archives-ouvertes.fr/hal-03058652)

<a id="wachter2019">[10]</a> 
Sandra Wachter and Brent Mittelstadt. 2019. 
Right to Reasonable Inferences: Re-Thinking Data Protection Law in the Age of Big Data and AI. 
Columbia Business Law Review 2 (2019), 494–620.

<a id="morley2020">[11]</a> 
Jessica Morley, Luciano Floridi, Libby Kinsey, and Anat Elhalal. 2020. 
From What to How: An Initial Review of Publicly Available AI Ethics Tools, Methods and Research to Translate Principles into Practices. 
Science and Engineering Ethics 26 (2020), 2141–2168. [https://doi.org/10.1007/s11948-019-00165-5](https://doi.org/10.1007/s11948-019-00165-5)

<a id="googleai2018">[12]</a> 
Google. 2018. 
AI at Google: Our Principles. 
[https://www.blog.google/technology/ai/ai-principles/](https://www.blog.google/technology/ai/ai-principles/)

<a id="europeancommissionEthicsAI2019">[13]</a> 
European Commission. 2019. 
Ethics guidelines for trustworthy AI. 
[https://www.aepd.es/sites/default/files/2019-12/ai-ethics-guidelines.pdf](https://www.aepd.es/sites/default/files/2019-12/ai-ethics-guidelines.pdf)

<a id="xiong2021">[14]</a> 
Pulei Xiong, Scott Buffett, Shahrear Iqbal, Philippe Lamontagne, Mohammad Mamun, and Heather Molyneaux. 2021. 
Towards a Robust and Trustworthy Machine Learning System Development. (1 2021).

<a id="biggio2018">[15]</a> 
Battista Biggio and Fabio Roli. 2018. 
Wild patterns: Ten years after the rise of adversarial machine learning. 
Pattern Recognition 84 (12 2018), 317–331. [https://doi.org/10.1016/j.patcog.2018.07.023](https://doi.org/10.1016/j.patcog.2018.07.023)

<a id="cretu2008">[16]</a> 
Gabriela F. Cretu, Angelos Stavrou, Michael E. Locasto, Salvatore J. Stolfo, and Angelos D. Keromytis. 2008. 
Casting out Demons: Sanitizing Training Data for Anomaly Sensors.
In 2008 IEEE Symposiumon Security and Privacy (sp2008).IEEE,81–95. [https://doi.org/10.1109/SP.2008.11](https://doi.org/10.1109/SP.2008.11)

<a id="globerson2006">[17]</a> 
Amir Globerson and Sam Roweis. 2006. 
Nightmare at test time. 
In Proceedings of the 23rd international conference on Machine learning - ICML ’06. ACMPress, New York, NewYork, USA, 353–360. [https://doi.org/10.1145/1143844.1143889](https://doi.org/10.1145/1143844.1143889)

<a id="lyu2015">[18]</a> 
Chunchuan Lyu, Kaizhu Huang, and Hai-Ning Liang. 2015. 
A Unified Gradient Regularization Family for Adversarial Examples. 
In 2015 IEEE International Conference on Data Mining.IEEE,301–309. [https://doi.org/10.1109/ICDM.2015.84](https://doi.org/10.1109/ICDM.2015.84)

<a id="goodfellow2014">[19]</a> 
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. 
Explaining and Harnessing Adversarial Examples. (12 2014).

<a id="papernot2016">[20]</a> 
Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. 2016. 
Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks. 
In 2016 IEEE Symposium on Security and Privacy (SP). IEEE, 582–597. [https://doi.org/10.1109/SP.2016.41](https://doi.org/10.1109/SP.2016.41)

<a id="nasr2018">[21]</a> 
Milad Nasr, Reza Shokri, and Amir Houmansadr. 2018. 
Machine Learning with Membership Privacy using Adversarial Regularization. (7 2018).

<a id="shokri2017">[22]</a> 
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2016. 
Membership Inference Attacks against Machine Learning Models. (10 2016).

<a id="kaya2020">[23]</a> 
Yigitcan Kaya, Sanghyun Hong, and Tudor Dumitras. 2020. 
On the Effectiveness of Regularization Against Membership Inference Attacks. (6 2020).

<a id="ye2021">[24]</a> 
Jiayuan Ye, Aadyaa Maddi, Sasi Kumar Murakonda, and Reza Shokri. 2021. 
Enhanced Membership Inference Attacks against Machine Learning Models. (11 2021).

<a id="dwork2006">[25]</a> 
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. 
Calibrating Noise to Sensitivity in Private Data Analysis. 265–284.
[https://doi.org/10.1007/11681878_14](https://doi.org/10.1007/11681878_14)

<a id="jia2019">[26]</a> 
Jinyuan Jia, Ahmed Salem, Michael Backes, Yang Zhang, and Neil Zhenqiang Gong. 2019. 
MemGuard: Defending against Black-Box Membership Inference Attacks via Adversarial Examples. (9 2019).

<a id="bihrane2021">[27]</a> 
Abeba Birhane, Pratyusha Kalluri, Dallas Card, William Agnew, Ravit Dotan, and Michelle Bao. 2021. 
The Values Encoded in Machine Learning Research. (6 2021).

<a id="mitchell2019">[28]</a> 
Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2018. 
Model Cards for Model Reporting. (10 2018). [https://doi.org/10.1145/3287560.3287596](https://doi.org/10.1145/3287560.3287596)

<a id="wexler2019">[29]</a> 
James Wexler, Mahima Pushkarna, Tolga Bolukbasi, Martin Wattenberg, Fernanda Viegas, and Jimbo Wilson. 2019. 
The What-If Tool: Interactive Probing of Machine Learning Models.(7 2019). [https://doi.org/10.1109/TVCG.2019.2934619](https://doi.org/10.1109/TVCG.2019.2934619)

<a id="bird2020">[30]</a> 
Sarah Bird, Miro Dudík, Richard Edgar, Brandon Horn, Roman Lutz, Vanessa Milan, Mehrnoosh Sameki, Hanna Wallach, and Kath- leen Walker. 2020. 
Fairlearn: A toolkit for assessing and improving fairness in AI. 
Technical Report MSR-TR-2020-32. Microsoft. [https://www.microsoft.com/en-us/research/publication/fairlearn-a-toolkit-for-assessing-and-improving-fairness-in-ai/](https://www.microsoft.com/en-us/research/publication/fairlearn-a-toolkit-for-assessing-and-improving-fairness-in-ai/)

<a id="garciamartin2019">[31]</a> 
Eva García-Martín, Crefeda Faviola Rodrigues, Graham Riley, and Håkan Grahn. 2019. 
Estimation of energy consumption in machine learning. 
J.Parallel and Distrib.Comput. 134(12 2019),75–88. [https://doi.org/10.1016/j.jpdc.2019.07.007](https://doi.org/10.1016/j.jpdc.2019.07.007)

<a id="gao2020">[32]</a> 
Yanjie Gao, Yu Liu, Hongyu Zhang, Zhengxian Li, Yonghao Zhu, Haoxiang Lin, and Mao Yang. 2020. 
Estimating GPU memory consumption of deep learning models. 
In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. ACM, NewYork, NY, USA, 1342–1352. [https://doi.org/10.1145/3368089.3417050](https://doi.org/10.1145/3368089.3417050)

<a id="mahendran2021">[33]</a> 
N. Mahendran. 2021. 
Analysis of memory consumption by neural networks based on hyperparameters. (10 2021).

<a id="assran2020">[34]</a> 
Mahmoud Assran, Joshua Romoff, Nicolas Ballas, Joelle Pineau, and Michael Rabbat. 2019. 
Gossip-based Actor-Learner Architectures for Deep Reinforcement Learning. (6 2019).

<a id="dalton2020">[35]</a> 
Steven Dalton, Iuri Frosio, and Michael Garland. 2019. 
Accelerating Reinforcement Learning through GPU Atari Emulation. (7 2019).

<a id="chasalow2021">[36]</a> 
Kyla Chasalow and Karen Levy. 2021. 
Representativeness in Statistics, Politics, and Machine Learning. 
In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT ’21). Association for Computing Machinery, New York, NY, USA, 77–89. [https://doi.org/10.1145/3442188.3445872](https://doi.org/10.1145/3442188.3445872)

<a id="abebe2020">[37]</a> 
Rediet Abebe, Solon Barocas, Jon Kleinberg, Karen Levy, Manish Raghavan, and David G. Robinson. 2020. 
Roles for computing in so- cial change. 
In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. ACM, New York, NY, USA, 252–260. [https://doi.org/10.1145/3351095.3372871](https://doi.org/10.1145/3351095.3372871)

<a id="keyes2019">[38]</a> 
Os Keyes, Jevan Hutson, and Meredith Durbin. 2019. 
A Mulching Proposal: Analysing and Improving an Algorithmic System for Turning the Elderly into High-Nutrient Slurry. 
In Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems (CHI EA ’19). Association for Computing Machinery, New York, NY, USA,1–11. [https://doi.org/10.1145/3290607.3310433](https://doi.org/10.1145/3290607.3310433)

<a id="floridi2018">[39]</a> 
Luciano Floridi, Josh Cowls, Monica Beltrametti, Raja Chatila, Patrice Chazerand, Virginia Dignum, Christoph Luetge, Robert Madelin, Ugo Pagallo, Francesca Rossi, Burkhard Schafer, Peggy Valcke, and Effy Vayena. 2018. 
AI4People—An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations. 
Minds and Machines 28, 4 (12 2018). 

<a id="bender2021">[40]</a> 
Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. 
On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?. 
In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT ’21). Association for Computing Machinery,NewYork,NY,USA,610–623. [https://doi.org/10.1145/3442188.3445922](https://doi.org/10.1145/3442188.3445922)

<a id="frenchminister2019">[41]</a> 
Mission assigned by the French Prime Minister. 2019. 
For a Meaningful Artificial Intelligence: Toward a French and European Strategy. [https://www.aiforhumanity.fr/pdfs/MissionVillani_Report_ENG-VF.pdf](https://www.aiforhumanity.fr/pdfs/MissionVillani_Report_ENG-VF.pdf)

<a id="raji2020">[42]</a> 
Inioluwa Deborah Raji, Andrew Smart, Rebecca N. White, Margaret Mitchell, Timnit Gebru, Ben Hutchinson, Jamila Smith-Loud, Daniel Theron, and Parker Barnes. 2020. 
Closing the AI accountability gap. 
In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. ACM,NewYork,NY,USA,33–44. [https://doi.org/10.1145/3351095.3372873](https://doi.org/10.1145/3351095.3372873)

<a id="mehrabi2021">[43]</a> 
Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. 2021. 
A Survey on Bias and Fairness in Machine Learning. 
ACM Comput. Surv. 54, 6 (7 2021). [https://doi.org/10.1145/3457607](https://doi.org/10.1145/3457607)

<a id="barredoarrieta2020">[44]</a> 
Alejandro Barredo Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. 
Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. 
Information Fusion 58 (6 2020), 82–115. [https://doi.org/10.1016/J.INFFUS.2019.12.012](https://doi.org/10.1016/J.INFFUS.2019.12.012)

<a id="kusner2017">[45]</a> 
Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. 2017. 
Counterfactual Fairness. 
In Advances in Neural Information Processing Systems, I Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, and R Garnett (Eds.), Vol. 30. Curran Associates, Inc. [https://proceedings.neurips.cc/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf](https://proceedings.neurips.cc/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf)

<a id="harrison2020">[46]</a> 
Galen Harrison, Julia Hanson, Christine Jacinto, Julio Ramirez, and Blase Ur. 2020. 
An empirical study on the perceived fairness of realistic, imperfect machine learning models. 
In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. ACM, New York, NY, USA, 392–402. [https://doi.org/10.1145/3351095.3372831](https://doi.org/10.1145/3351095.3372831)

<a id="srivastava2019">[47]</a> 
Megha Srivastava, Hoda Heidari, and Andreas Krause. 2019. Mathematical Notions vs. 
Human Perception of Fairness. 
In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, New York, NY, USA, 2459–2468. [https://doi.org/10.1145/3292500.3330664](https://doi.org/10.1145/3292500.3330664)

<a id="kearns2018">[48]</a> 
Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. 2017. 
Fairness in Criminal Justice Risk Assessments: The State of the Art. (3 2017).

<a id="verma2018">[49]</a> 
Sahil Verma and Julia Rubin. 2018. 
Fairness definitions explained. 
In Proceedings of the International Workshop on Software Fairness. ACM, New York, NY, USA,1–7. [https://doi.org/10.1145/3194770.3194776](https://doi.org/10.1145/3194770.3194776)

<a id="hardt2016">[50]</a> 
Moritz Hardt, Eric Price, and Nathan Srebro. 2016. 
Equality of Opportunity in Supervised Learning. 
In Proceedings of the 30th International Conference on Neural Information Processing Systems (NIPS’16). Curran Associates Inc., Red Hook, NY, USA, 3323–3331.

<a id="vanBerkel2021">[51]</a> 
Niels van Berkel, Jorge Goncalves, Daniel Russo, Simo Hosio, and Mikael B. Skov. 2021. 
Effect of Information Presentation on Fairness Perceptions of Machine Learning Predictors. 
In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. ACM, New York, NY, USA, 1–13. [https://doi.org/10.1145/3411764.3445365](https://doi.org/10.1145/3411764.3445365)

<a id="berk2017">[52]</a> 
Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. 2017. 
Fairness in Criminal Justice Risk Assessments: The State of the Art. (3 2017).

<a id="chouldechova2016">[53]</a> 
Alexandra Chouldechova. 2016. 
Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. (10 2016).

<a id="grgichlaca2018">[54]</a> 
Nina Grgic-Hlaca, Muhammad Bilal Zafar, Krishna P Gummadi, and Adrian Weller. 2018. 
Beyond Distributive Fairness in Algorithmic Decision Making: Feature Selection for Procedurally Fair Learning. 
In Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 32.


<a id="kleinberg2016">[55]</a> 
Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. 2016. 
Inherent Trade-Offs in the Fair Determination of Risk Scores. (9 2016).

<a id="wang2020">[56]</a> 
Zezhong Wang, Jacob Ritchie, Jingtao Zhou, Fanny Chevalier, and Benjamin Bach. 2021. 
Data Comics for Reporting Controlled User Studies in Human-Computer Interaction.
IEEE Transactionson Visualization and Computer Graphics 27, 2 (2 2021),967–977. [https://doi.org/10.1109/TVCG.2020.3030433](https://doi.org/10.1109/TVCG.2020.3030433)

<a id="saleiro2018">[57]</a> 
Pedro Saleiro, Benedict Kuester, Loren Hinkson, Jesse London, Abby Stevens, Ari Anisfeld, Kit T Rodolfa, and Rayid Ghani. 2018. 
Aequitas: A Bias and Fairness Audit Toolkit.

<a id="dixon2018">[58]</a> 
Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. 2018. 
Measuring and Mitigating Unintended Bias in Text Classification.
In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society. ACM, New York, NY, USA, 67–73. [https://doi.org/10.1145/3278721.3278729](https://doi.org/10.1145/3278721.3278729)

<a id="bellamy2018">[59]</a> 
R K E Bellamy, K Dey, M Hind, S C Hoffman, S Houde, K Kannan, P Lohia, J Martino, S Mehta, A Mojsilović, S Nagar, K Natesan Ramamurthy, J Richards, D Saha, P Sattigeri, M Singh, K R Varshney, and Y Zhang. 2019. 
AI Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias. 
IBM Journal of Research and Development 63, 4/5 (2019), 1–4. [https://doi.org/10.1147/JRD.2019.2942287](https://doi.org/10.1147/JRD.2019.2942287)

<a id="stuartgeiger2020">[60]</a> 
R Stuart Geiger, Kevin Yu, Yanlai Yang, Mindy Dai, Jie Qiu, Rebekah Tang, and Jenny Huang. 2020. 
Garbage in, Garbage out? Do Machine Learning Application Papers in Social Computing Report Where Human-Labeled Training Data Comes From?. 
In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT* ’20). Association for Computing Machinery, New York, NY, USA, 325–336. [https://doi.org/10.1145/3351095.3372862](https://doi.org/10.1145/3351095.3372862)

<a id="paullada2020">[61]</a> 
Amandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, Emily Denton, and Alex Hanna. 2020. 
Data and its (dis)contents: A survey of dataset development and use in machine learning research.(12 2020). [http://arxiv.org/abs/2012.05345](http://arxiv.org/abs/2012.05345)

<a id="gebru2020">[62]</a> 
Timnit Gebru, Google Jamie Morgenstern, Briana Vecchione, and Jennifer Wortman Vaughan. 2020. 
Datasheets for Datasets.

<a id="lee2019">[63]</a> 
Min Kyung Lee, Daniel Kusbit, Anson Kahng, Ji Tae Kim, Xinran Yuan, Allissa Chan, Daniel See, Ritesh Noothigattu, Siheon Lee, Alexandros Psomas, and Ariel D Procaccia. 2019. 
WeBuildAI: Participatory Framework for Algorithmic Governance. 
Proc. ACM Hum.-Comput. Interact. 3, CSCW (11 2019). [https://doi.org/10.1145/3359283](https://doi.org/10.1145/3359283)

<a id="zhouWeb">[64]</a> 
Angela Zhou, David Madras, Inioluwa Raji Raji, Bogdan Kulynych, Smitha Mili, and Richard Zemel. [n. d.]. 
Call for participation: Participatory Approaches to Machine Learning. [https://participatoryml.github.io/](https://participatoryml.github.io/)

<a id="royalsociety2019">[65]</a> 
The Royal Society. 2019. Explainable AI: the basics . 
[https://royalsociety-org.tudelft.idm.oclc.org/-/media/policy/projects/explainable-ai/AI-and-interpretability-policy-briefing.pdf](https://royalsociety-org.tudelft.idm.oclc.org/-/media/policy/projects/explainable-ai/AI-and-interpretability-policy-briefing.pdf)

<a id="bender2018">[66]</a> 
Emily M. Bender and Batya Friedman. 2018. 
Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science.
Transactions of the Association for Computational Linguistics 6 (12 2018). [https://doi.org/10.1162/tacl_a_00041](https://doi.org/10.1162/tacl_a_00041)

<a id="OECD2019">[67]</a> 
OECD. 2019. 
Recommendation of the Council on Artificial Intelligence. 
[https://legalinstruments.oecd.org/en/instruments/OECD-LEGAL-0406](https://legalinstruments.oecd.org/en/instruments/OECD-LEGAL-0406)

<a id="balayn2021Edri">[68]</a> 
Agathe Balayn and Seda Gürses. 2021. 
Beyond Debiasing: Regulating AI and its inequalities. 
[https://edri.org/our-work/if-ai-is-the-problem-is-debiasing-the-solution/](https://edri.org/our-work/if-ai-is-the-problem-is-debiasing-the-solution/)

<a id="kyunglee2017">[69]</a> 
Min Kyung Lee and Su Baykal. 2017. 
Algorithmic Mediation in Group Decisions: Fairness Perceptions of Algorithmically Mediated vs. Discussion-Based Social Division. 
In Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing (CSCW’17).Association for Computing Machinery, NewYork, NY, USA, 1035–1048. [https://doi.org/10.1145/2998181.2998230](https://doi.org/10.1145/2998181.2998230)

<a id="alfrink2020">[70]</a> 
Kars Alfrink, T. Turel, A. I. Keller, N. Doorn, and G. W. Kortuem. 2020. 
Contestable City Algorithms. 
International Conference on Machine Learning Workshop.

<a id="kalluri2020">[71]</a> 
Pratyusha Kalluri. 2020. 
Don’t ask if artificial intelligence is good or fair, ask how it shifts power. 
Nature 583, 7815 (2020). [https://doi.org/10.1038/d41586-020-02003-2](https://doi.org/10.1038/d41586-020-02003-2)

<a id="lyons2021">[72]</a> 
Henrietta Lyons, Eduardo Velloso, and Tim Miller. 2021. 
Conceptualising Contestability: Perspectives on Contesting Algorithmic Decisions. (2 2021). [https://doi.org/10.1145/3449180](https://doi.org/10.1145/3449180)

<a id="hirsch2017">[73]</a> 
Tad Hirsch, Kritzia Merced, Shrikanth Narayanan, Zac E. Imel, and David C. Atkins. 2017. 
Designing Contestability. 
In Proceedings of the 2017 Conference on Designing Interactive Systems. ACM, New York, NY, USA. [https://doi.org/10.1145/3064663.3064703](https://doi.org/10.1145/3064663.3064703)

<a id="mitra2021">[74]</a> 
Tanushree Mitra. 2021. 
Provocation: Contestability in Large-Scale Interactive {NLP} Systems. 
In Proceedings of the First Workshop on Bridging Human–Computer Interaction and Natural Language Processing. Association for Computational Linguistics, 96–100.

<a id="teliaai2019">[75]</a> 
Telia Company. 2019. 
Guiding Principles on Trusted AI Ethics. 
[https://www.teliacompany.com/globalassets/telia-company/documents/about-telia-company/public-policy/2018/guiding-principles-on-trusted-ai-ethics.pdf](https://www.teliacompany.com/globalassets/telia-company/documents/about-telia-company/public-policy/2018/guiding-principles-on-trusted-ai-ethics.pdf)


<a id="suresh2021">[76]</a> 
Harini Suresh, Steven R. Gomez, Kevin K. Nam, and Arvind Satyanarayan. 2021. 
Beyond Expertise and Roles: A Framework to Characterize the Stakeholders of Interpretable Machine Learning and their Needs. 
In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. ACM, NewYork, NY, USA, 1–16. [https://doi.org/10.1145/3411764.3445088](https://doi.org/10.1145/3411764.3445088)

<a id="henin2021">[77]</a> 
Clément Henin and Daniel Le Métayer. 2021. 
Beyond explainability: justifiability and contestability of algorithmic decision systems. 
AI & SOCIETY (7 2021). [https://doi.org/10.1007/s00146-021-01251-8](https://doi.org/10.1007/s00146-021-01251-8)

<a id="rossi2017">[78]</a> 
Arianna Rossi and Monica Palmirani. 2017. 
A Visualization Approach for Adaptive Consent in the European Data Protection Framework. 
In 2017 Conference for E-Democracy and Open Government (CeDEM). IEEE, 159–170. [https://doi.org/10.1109/CeDEM.2017.23](https://doi.org/10.1109/CeDEM.2017.23)

<a id="holtz2011">[79]</a> 
Leif-Erik Holtz, Katharina Nocun, and Marit Hansen. 2011. 
Towards Displaying Privacy Information with Icons. 338–348. [https://doi.org/10.1007/978-3-642-20769-3_27](https://doi.org/10.1007/978-3-642-20769-3_27)

<a id="zimmerman2014">[80]</a> 
Christian Zimmermann, Rafael Accorsi, and Gunter Muller. 2014. 
Privacy Dashboards: Reconciling Data-Driven Business Models and Privacy. 
In 2014 Ninth International Conference on Availability, Reliability and Security. IEEE, 152–157. [https://doi.org/10.1109/ARES.2014.27](https://doi.org/10.1109/ARES.2014.27)

<a id="earp2016">[81]</a> 
Julia Earp and Jessica Staddon. 2016. 
"I had no idea this was a thing". 
In Proceedings of the 6th Workshop on Socio-Technical Aspects in Security and
Trust. ACM, New York, NY, USA, 79–86. [https://doi.org/10.1145/3046055.3046062](https://doi.org/10.1145/3046055.3046062)

<a id="fischerhubner2016">[82]</a> 
Simone Fischer-Hübner, Julio Angulo, Farzaneh Karegar, and Tobias Pulls. 2016.
Transparency, Privacy and Trust – Technology for Tracking and Controlling My Data Disclosures: Does This Work? 3–14. [https://doi.org/10.1007/978-3-319-41354-9_1](https://doi.org/10.1007/978-3-319-41354-9_1)


<a id="herder2020">[83]</a> 
Eelco Herder and Olaf van Maaren. 2020. 
Privacy Dashboards: The Impact of the Type of Personal Data and User Control on Trust and Perceived Risk. 
In Adjunct Publication of the 28th ACM Conference on User Modeling, Adaptation and Personalization. ACM, New York, NY, USA, 169–174.
[https://doi.org/10.1145/3386392.3399557](https://doi.org/10.1145/3386392.3399557)

<a id="farke2021">[84]</a> 
Florian M. Farke, David G. Balash, Maximilian Golla, Markus Dürmuth, and Adam J. Aviv. 2021. 
Are Privacy Dashboards Good for End Users?
Evaluating User Perceptions and Reactions to Google’s My Activity (Extended Version). (5 2021).

<a id="krafft2019">[85]</a> 
TD Krafft and K Zweig. 2019. 
Transparenz und Nachvollziehbarkeit algorithmenbasierter Entscheidungsprozesse. Ein Regulierungsvorschlag (2019).

<a id="amershi2015">[86]</a> 
Saleema Amershi, Max Chickering, Steven M. Drucker, Bongshin Lee, Patrice Simard, and Jina Suh. 2015. ModelTracker. 
In Proceedings of the 33rd
Annual ACM Conference on Human Factors in Computing Systems. ACM, New York, NY, USA. [https://doi.org/10.1145/2702123.2702509](https://doi.org/10.1145/2702123.2702509)

<a id="mishra2021">[87]</a> 
Swati Mishra and Jeffrey M Rzeszotarski. 2021. 
Designing Interactive Transfer Learning Tools for ML Non-Experts. 
In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. ACM, New York, NY, USA. [https://doi.org/10.1145/3411764.3445096](https://doi.org/10.1145/3411764.3445096)

<a id="veraliao2021">[88]</a> 
Q. Vera Liao, Milena Pribić, Jaesik Han, Sarah Miller, and Daby Sow. 2021. 
Question-Driven Design Process for Explainable AI User Experiences. (4 2021).

<a id="anik2021">[89]</a> 
Ariful Islam Anik and Andrea Bunt. 2021. 
Data-Centric Explanations: Explaining Training Data of Machine Learning Systems to Promote Transparency. 
In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. ACM, New York, NY, USA. [https://doi.org/10.1145/3411764.3445736](https://doi.org/10.1145/3411764.3445736)

<a id="cai2019">[90]</a> 
Carrie J. Cai, Jonas Jongejan, and Jess Holbrook. 2019. 
The effects of example-based explanations in a machine learning interface. 
In Proceedings of the 24th International Conference on Intelligent User Interfaces. ACM, New York, NY, USA. [https://doi.org/10.1145/3301275.3302289](https://doi.org/10.1145/3301275.3302289)

<a id="veraliao2020">[91]</a> 
Q. Vera Liao, Daniel Gruen, and Sarah Miller. 2020. 
Questioning the AI: Informing Design Practices for Explainable AI User Experiences. (1 2020). [https://doi.org/10.1145/3313831.3376590](https://doi.org/10.1145/3313831.3376590)

<a id="jin2021">[92]</a> 
Weina Jin, Jianyu Fan, Diane Gromala, Philippe Pasquier, and Ghassan Hamarneh. 2021. EUCA: A Practical Prototyping Framework towards End-User-Centered Explainable Artificial Intelligence. (2 2021). 
[https://arxiv.org/abs/2102.02437](https://arxiv.org/abs/2102.02437)

<a id="dodge2019">[93]</a> 
Jonathan Dodge, Q. Vera Liao, Yunfeng Zhang, Rachel K. E. Bellamy, and Casey Dugan. 2019. 
Explaining Models: An Empirical Study of How Explanations Impact Fairness Judgment. (1 2019). [https://doi.org/10.1145/3301275.3302310](https://doi.org/10.1145/3301275.3302310)

<a id="binns2018">[94]</a> 
Reuben Binns, Max Van Kleek, Michael Veale, Ulrik Lyngs, Jun Zhao, and Nigel Shadbolt. 2018. 
’It’s Reducing a Human Being to a Percentage’;
Perceptions of Justice in Algorithmic Decisions. (1 2018). [https://doi.org/10.1145/3173574.3173951](https://doi.org/10.1145/3173574.3173951)

<a id="alqaraawi2020">[95]</a> 
Ahmed Alqaraawi, Martin Schuessler, Philipp Weiß, Enrico Costanza, and Nadia Berthouze. 2020. 
Evaluating saliency map explanations for convolutional neural networks. 
In Proceedings of the 25th International Conference on Intelligent User Interfaces. ACM, New York, NY, USA.
[https://doi.org/10.1145/3377325.3377519](https://doi.org/10.1145/3377325.3377519)

<a id="dhurandhar2018">[96]</a> 
Amit Dhurandhar, Pin-Yu Chen, Ronny Luss, Chun-Chen Tu, Paishun Ting, Karthikeyan Shanmugam, and Payel Das. 2018. 
Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives. (2 2018).

<a id="mothilal2019">[97]</a> 
Ramaravind Kommiya Mothilal, Amit Sharma, and Chenhao Tan. 2019. 
Explaining Machine Learning Classifiers through Diverse Counterfactual
Explanations. (5 2019). [https://doi.org/10.1145/3351095.3372850](https://doi.org/10.1145/3351095.3372850)

<a id="long2020">[98]</a> 
Duri Long and Brian Magerko. 2020. What is AI Literacy? Competencies and Design Considerations. 
In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. Association for Computing Machinery, New York, NY, USA, 1–16. 
[https://doi-org.tudelft.idm.oclc.org/10.1145/3313831.3376727](https://doi-org.tudelft.idm.oclc.org/10.1145/3313831.3376727)

<a id="hemment2019">[99]</a> 
Drew Hemment, Ruth Aylett, Vaishak Belle, Dave Murray-Rust, Ewa Luger, Jane Hillston, Michael Rovatsos, and Frank Broz. 2019. 
Experiential AI. AI Matters 5, 1 (4 2019), 25–31. [https://doi.org/10.1145/3320254.3320264](https://doi.org/10.1145/3320254.3320264)

<a id="kluttz2018">[100]</a> 
Daniel Kluttz, Nitin Kohli, and Deirdre K. Mulligan. 2018. 
Contestability and Professionals: From Explanations to Engagement with Algorithmic Systems. SSRN Electronic Journal (2018). [https://doi.org/10.2139/ssrn.3311894](https://doi.org/10.2139/ssrn.3311894)

<a id="calvert2019">[101]</a> 
 Simeon C Calvert, Daniël D Heikoop, Giulio Mecacci, and Bart Van Arem. 2019.A human centric framework for the analysis of automated driving
systems based on meaningful human control. 
Theoretical Issues in Ergonomics Science 21, 4 (2019), 478–506. 
[https://doi.org/10.1080/1463922X.2019.1697390](https://doi.org/10.1080/1463922X.2019.1697390)

<a id="hong2021">[102]</a> 
Matthew K. Hong, Adam Fourney, Derek DeBellis, and Saleema Amershi. 2021. Planning for Natural Language Failures with the AI Playbook. 
In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. ACM, New York, NY, USA, 1–11. [https://doi.org/10.1145/3411764.3445735](https://doi.org/10.1145/3411764.3445735)

<a id="nori2019">[103]</a> 
Harsha Nori, Samuel Jenkins, Paul Koch, and Rich Caruana. 2019. 
InterpretML: A Unified Framework for Machine Learning Interpretability. (9
2019).

<a id="henderson2020">[104]</a> 
Peter Henderson, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky, and Joelle Pineau. 2020. 
Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning. (1 2020).

<a id="vaccaro2020">[105]</a> 
Kristen Vaccaro, Christian Sandvig, and Karrie Karahalios. 2020. 
"At the End of the Day Facebook Does What ItWants". Proceedings of the ACM on
Human-Computer Interaction 4, CSCW2 (10 2020), 1–22. [https://doi.org/10.1145/3415238](https://doi.org/10.1145/3415238)



[Back to main page](index.md)
